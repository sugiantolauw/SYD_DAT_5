{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Science with Python \n",
    "## General Assembly\n",
    "## Natural Language Processing (NLP)\n",
    "\n",
    "Make sure you have installed nltk and downloaded the following copora:\n",
    "\n",
    "* punkt\n",
    "* gutenberg\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Part 1\n",
    "\n",
    "###Tokenization\n",
    "\n",
    "What:  Separate text into units such as sentences or words\n",
    "\n",
    "Why:   Gives structure to previously unstructured text\n",
    "\n",
    "Notes: Relatively easy with English language text, not easy with some languages\n",
    "\n",
    "\n",
    "\"corpus\" = collection of documents\n",
    "\n",
    "\"corpora\" = plural form of corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'austen-emma.txt',\n",
       " u'austen-persuasion.txt',\n",
       " u'austen-sense.txt',\n",
       " u'bible-kjv.txt',\n",
       " u'blake-poems.txt',\n",
       " u'bryant-stories.txt',\n",
       " u'burgess-busterbrown.txt',\n",
       " u'carroll-alice.txt',\n",
       " u'chesterton-ball.txt',\n",
       " u'chesterton-brown.txt',\n",
       " u'chesterton-thursday.txt',\n",
       " u'edgeworth-parents.txt',\n",
       " u'melville-moby_dick.txt',\n",
       " u'milton-paradise.txt',\n",
       " u'shakespeare-caesar.txt',\n",
       " u'shakespeare-hamlet.txt',\n",
       " u'shakespeare-macbeth.txt',\n",
       " u'whitman-leaves.txt']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the NLTK library, and use ntlk.corpus.gutenberg.fileids() to\n",
    "# find the filenames for Jane Austen's Emma and Lewis Carrol's Alice in \n",
    "# Wonderland\n",
    "\n",
    "import nltk\n",
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'[', u'Alice', u\"'\", u's', u'Adventures', u'in', u'Wonderland', u'by', u'Lewis', u'Carroll', u'1865', u']'], [u'CHAPTER', u'I', u'.'], ...]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Break these novels up into sentences. Put these sentence lists into\n",
    "# a list so that you can use it later\n",
    "nltk.corpus.gutenberg.raw('austen-emma.txt')\n",
    "nltk.corpus.gutenberg.sents('austen-emma.txt')\n",
    "nltk.corpus.gutenberg.raw('carroll-alice.txt')\n",
    "nltk.corpus.gutenberg.sents('carroll-alice.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Count the number of sentences in each novel.\n",
    "emma_sentence_count = len(nltk.corpus.gutenberg.sents('austen-emma.txt'))\n",
    "alice_sentence_count = len(nltk.corpus.gutenberg.sents('carroll-alice.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Break each sentence up into words. You will end up with a \n",
    "# list of lists of words for Emma and another one for Alice in\n",
    "# Wonderland\n",
    "emma_words_count = len(nltk.corpus.gutenberg.words(\"austen-emma.txt\"))\n",
    "alice_words_count = len(nltk.corpus.gutenberg.words(\"carroll-alice.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7338\n",
      "2631\n"
     ]
    }
   ],
   "source": [
    "# Count the number of words in each sentence\n",
    "#drop all things that are not words\n",
    "def is_a_word(x):\n",
    "    return x not in \".,[]:\"\n",
    "\n",
    "cleaned_up_austen = [\n",
    "    x.lower() \n",
    "        for x in nltk.corpus.gutenberg.words(\"austen-emma.txt\")\n",
    "        if is_a_word(x)\n",
    "]\n",
    "\n",
    "cleaned_up_alice = [\n",
    "    x.lower() \n",
    "        for x in nltk.corpus.gutenberg.words(\"carroll-alice.txt\")\n",
    "        if is_a_word(x)\n",
    "]\n",
    "\n",
    "emma_words = set(cleaned_up_austen)\n",
    "alice_words = set(cleaned_up_alice)\n",
    "\n",
    "print len(emma_words)\n",
    "print len (alice_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "# Which novel has more average words per sentence?\n",
    "# Given their target audience, is this what you would expect?\n",
    "emma_average = emma_words_count / emma_sentence_count\n",
    "print emma_average\n",
    "alice_average = alice_words_count / alice_sentence_count\n",
    "print alice_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a flat list (i.e. not a list of lists) of words in\n",
    "# the two novels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For each novel, construct a set of all the distinct words used\n",
    "emma_words_set = set(emma_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alice_words_set = set(alice_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0421973927095\n",
      "0.0845083994475\n"
     ]
    }
   ],
   "source": [
    "# Calculate the lexical diversity of each novel (distinct words / word count)\n",
    "emma_diversity = float(len(emma_words_set)) / len(cleaned_up_austen)\n",
    "print emma_diversity\n",
    "alice_diversity = float(len(alice_words_set)) / len(cleaned_up_alice)\n",
    "print alice_diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (Optional, only for the very keen)\n",
    "# Repeat the above analysis for all the Gutenberg samples\n",
    "# Create a dataframe with the names of the novels, when they were written,\n",
    "# whether they were for children, the lexical diversity and the average sentence length.\n",
    "# Can you use logistic regression to predict the audience, based on the content?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Part 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make nltk.Text objects from the two novels\n",
    "\n",
    "emma_text_obj = nltk.Text(nltk.corpus.gutenberg.words('austen-emma.txt'))\n",
    "alice_text_obj = nltk.Text(nltk.corpus.gutenberg.words('carroll-alice.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 192 matches:\n",
      "marriage . A worthy employment for a young lady ' s mind ! But if , which I rat\n",
      "ice .\" \" Mr . Elton is a very pretty young man , to be sure , and a very good y\n",
      "g man , to be sure , and a very good young man , and I have a great regard for \n",
      "hildren of their own , nor any other young creature of equal kindred to care fo\n",
      "is fond report of him as a very fine young man had made Highbury feel a sort of\n",
      "formed a very favourable idea of the young man ; and such a pleasing attention \n",
      " . Knightley ; and by Mr . Elton , a young man living alone without liking it ,\n",
      "ee of popularity for a woman neither young , handsome , rich , nor married . Mi\n",
      "nciples and new systems -- and where young ladies for enormous pay might be scr\n",
      "was no wonder that a train of twenty young couple now walked after her to churc\n",
      " a long visit in the country to some young ladies who had been at school there \n",
      " Harriet Smith ' s being exactly the young friend she wanted -- exactly the som\n",
      "was a single man ; that there was no young Mrs . Martin , no wife in the case ;\n",
      "hout having any idea of his name . A young farmer , whether on horseback or on \n",
      "oubt of his being a very respectable young man . I know , indeed , that he is s\n",
      "ly four - and - twenty . That is too young to settle . His mother is perfectly \n",
      "if he could meet with a good sort of young woman in the same rank as his own , \n",
      "w no alarming symptoms of love . The young man had been the first admirer , but\n",
      "neat , and he looked like a sensible young man , but his person had no other ad\n",
      "n life seem to allow it ; but if any young man were to set about copying him , \n",
      "erable . On the contrary , I think a young man might be very safely recommended\n",
      "son fixed on by Emma for driving the young farmer out of Harriet ' s head . She\n",
      "oured , well - meaning , respectable young man , without any deficiency of usef\n",
      " . And he was really a very pleasing young man , a young man whom any woman not\n",
      "really a very pleasing young man , a young man whom any woman not fastidious mi\n",
      "None\n",
      "Displaying 5 of 5 matches:\n",
      " ' Hold your tongue , Ma !' said the young Crab , a little snappishly . ' You '\n",
      " You are old , Father William ,' the young man said , ' And your hair has becom\n",
      "m getting tired of this . I vote the young lady tells us a story .' ' I ' m afr\n",
      " !' said the Queen , ' and take this young lady to see the Mock Turtle , and to\n",
      "ars , but said nothing . ' This here young lady ,' said the Gryphon , ' she wan\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Does Jane Austen ever mention the word 'young' in Emma? What about Lewis Carroll?\n",
    "print emma_text_obj.concordance('Young')\n",
    "print alice_text_obj.concordance('Young')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the_man here_lady the_crab the_lady this_lady\n",
      "None\n",
      "accomplished_woman worthy_man the_farmer the_man unexceptionable_man\n",
      "a_man so_as amiable_man pretty_woman the_are no_mrs too_: pert_lawyer\n",
      "of_person alarming_man ,_cox a_woman too_; the_woman of_men\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# What are the common contexts for these words?\n",
    "print alice_text_obj.common_contexts(['Young'])\n",
    "print emma_text_obj.common_contexts(['Young'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEZCAYAAACervI0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDyUlFQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x817b438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Where does the word 'cat' appear in Alice and Wonderland?\n",
    "%matplotlib inline\n",
    "alice_text_obj.dispersion_plot(['cat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Part 3\n",
    "\n",
    "###Stemming\n",
    "What:  Reduce a word to its base/stem form\n",
    "\n",
    "Why:   Often makes sense to treat multiple word forms the same way\n",
    "\n",
    "Notes: Uses a \"simple\" and fast rule-based approach\n",
    "       Output can be undesirable for irregular words\n",
    "       Stemmed words are usually not shown to users (used for analysis/indexing)\n",
    "       Some search engines treat words with the same stem as synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create an English stemmer that uses the Snowball technique\n",
    "# nltk.stem.snowball.SnowballStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stem the following words: charge, charging, charged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Can you stem \"words\" with punctuation in them? Or which have no letters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a new list of words from the novels by dropping out spurious non-words.\n",
    "# You might find word_is_just_letters() helpful\n",
    "def word_is_just_letters(word):\n",
    "    import re\n",
    "    return re.search('^[a-zA-Z]+', word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stem all those words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create two collections.Counter objects (one for each novel)\n",
    "# so that you can easily count word stems. If you give\n",
    "# the stemmed lists as an argument to constructor, \n",
    "# you can use .most_common(25) to get the top 25 tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Lemmatization / synset\n",
    "What:  Derive the canonical form ('lemma') of a word\n",
    "    \n",
    "Why:   Can be better than stemming, reduces words to a 'normal' form.\n",
    "    \n",
    "Notes: Uses a dictionary-based approach (slower than stemming)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# What synsets does 'dog' belong to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Which synset is the one you were thinking of?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# What is its hypernym?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# What about wolves? What synsets does it belong to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# How closely related are those concepts (dogs and wolves)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# How closely related are the concepts 'dog' and 'novel'?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Part 3 Part of speech tagging\n",
    "\n",
    "Other:\n",
    "- Analysing data with the Alchemy API\n",
    "- Further Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Part of Speech Tagging\n",
    "\n",
    "What:  Determine the part of speech of a word\n",
    "    \n",
    "Why:   This can inform other methods and models such as Named Entity Recognition\n",
    "    \n",
    "Notes: http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use nltk.pos_tag to parse a sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (Optional for the enthusiastic)\n",
    "# What verbs did Jane Austen use a lot of?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Part 4\n",
    "###Stopword Removal\n",
    "\n",
    "What:  Remove common words that will likely appear in any text\n",
    "    \n",
    "Why:   They don't tell you much about your text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# most of top 25 stemmed tokens are \"worthless\"\n",
    "c.most_common(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# view the list of stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "sorted(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################\n",
    "### Exercise  ####\n",
    "##################\n",
    "\n",
    "\n",
    "# Create a variable called stemmed_stops which is the \n",
    "# stemmed version of each stopword in stopwords\n",
    "# Use the stemmer we used up above!\n",
    "\n",
    "# Then create a list called stemmed_tokens_no_stop that \n",
    "# contains only the tokens in stemmed_tokens that aren't in \n",
    "# stemmed_stops\n",
    "\n",
    "# Show the 25 most common stemmed non stop word tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Part 5\n",
    "###Named Entity Recognition\n",
    "\n",
    "What:  Automatically extract the names of people, places, organizations, etc.\n",
    "\n",
    "Why:   Can help you to identify \"important\" words\n",
    "\n",
    "Notes: Training NER classifier requires a lot of annotated training data\n",
    "       Should be trained on data relevant to your task\n",
    "       Stanford NER classifier is the \"gold standard\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = 'Ian is an instructor for General Assembly'\n",
    "\n",
    "tokenized = nltk.word_tokenize(sentence)\n",
    "\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagged = nltk.pos_tag(tokenized)\n",
    "\n",
    "tagged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chunks = nltk.ne_chunk(tagged)\n",
    "\n",
    "chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_entities(text):\n",
    "    entities = []\n",
    "    # tokenize into sentences\n",
    "    for sentence in nltk.sent_tokenize(text):\n",
    "        # tokenize sentences into words\n",
    "        # add part-of-speech tags\n",
    "        # use NLTK's NER classifier\n",
    "        chunks = nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sentence)))\n",
    "        # parse the results\n",
    "        entities.extend([chunk for chunk in chunks if hasattr(chunk, 'label')])\n",
    "    return entities\n",
    "\n",
    "for entity in extract_entities('Ian is an instructor for General Assembly'):\n",
    "    print '[' + entity.label() + '] ' + ' '.join(c[0] for c in entity.leaves())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Part 6\n",
    "###Term Frequency - Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "What:  Computes \"relative frequency\" that a word appears in a document\n",
    "           compared to its frequency across all documents\n",
    "\n",
    "Why:   More useful than \"term frequency\" for identifying \"important\" words in\n",
    "           each document (high frequency in that document, low frequency in\n",
    "           other documents)\n",
    "\n",
    "Notes: Used for search engine scoring, text summarization, document clustering\n",
    "\n",
    "How: \n",
    "    TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document).\n",
    "    IDF(t) = log_e(Total number of documents / Number of documents with term t in it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample = ['Bob likes sports', 'Bob hates sports', 'Bob likes likes trees']\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Each row represents a sentence\n",
    "# Each column represents a word\n",
    "vect.fit_transform(sample).toarray()\n",
    "vect.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf.fit_transform(sample).toarray()\n",
    "tfidf.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the IDF of each word\n",
    "idf = tfidf.idf_\n",
    "print dict(zip(tfidf.get_feature_names(), idf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############\n",
    "## Exercise ###\n",
    "###############\n",
    "\n",
    "\n",
    "# for each sentence in sample, find the most \"interesting \n",
    "#words\" by ordering their tfidf in ascending order\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Part 7\n",
    "\n",
    "###LDA - Latent Dirichlet Allocation\n",
    "\n",
    "What:  Way of automatically discovering topics from sentences\n",
    "\n",
    "Why:   Much quicker than manually creating and identifying topic clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lda\n",
    "\n",
    "# Instantiate a count vectorizer with two additional parameters\n",
    "vect = CountVectorizer(stop_words='english', ngram_range=[1,3]) \n",
    "sentences_train = vect.fit_transform(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instantiate an LDA model\n",
    "model = lda.LDA(n_topics=10, n_iter=500)\n",
    "model.fit(sentences_train) # Fit the model \n",
    "n_top_words = 10\n",
    "topic_word = model.topic_word_\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vect.get_feature_names())[np.argsort(topic_dist)][:-n_top_words:-1]\n",
    "    print('Topic {}: {}'.format(i, ', '.join(topic_words)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EXAMPLE: Automatically summarize a document\n",
    "\n",
    "\n",
    "# corpus of 2000 movie reviews\n",
    "from nltk.corpus import movie_reviews\n",
    "reviews = [movie_reviews.raw(filename) for filename in movie_reviews.fileids()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create document-term matrix\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "dtm = tfidf.fit_transform(reviews)\n",
    "features = tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find the most and least \"interesting\" sentences in a randomly selected review\n",
    "def summarize():\n",
    "    \n",
    "    # choose a random movie review    \n",
    "    review_id = np.random.randint(0, len(reviews))\n",
    "    review_text = reviews[review_id]\n",
    "\n",
    "    # we are going to score each sentence in the review for \"interesting-ness\"\n",
    "    sent_scores = []\n",
    "    # tokenize document into sentences\n",
    "    for sentence in nltk.sent_tokenize(review_text):\n",
    "        # exclude short sentences\n",
    "        if len(sentence) > 6:\n",
    "            score = 0\n",
    "            token_count = 0\n",
    "            # tokenize sentence into words\n",
    "            tokens = nltk.word_tokenize(sentence)\n",
    "            # compute sentence \"score\" by summing TFIDF for each word\n",
    "            for token in tokens:\n",
    "                if token in features:\n",
    "                    score += dtm[review_id, features.index(token)]\n",
    "                    token_count += 1\n",
    "            # divide score by number of tokens\n",
    "            sent_scores.append((score / float(token_count + 1), sentence))\n",
    "\n",
    "    # lowest scoring sentences\n",
    "    print '\\nLOWEST:\\n'\n",
    "    for sent_score in sorted(sent_scores)[:3]:\n",
    "        print sent_score[1]\n",
    "\n",
    "    # highest scoring sentences\n",
    "    print '\\nHIGHEST:\\n'\n",
    "    for sent_score in sorted(sent_scores, reverse=True)[:3]:\n",
    "        print sent_score[1]\n",
    "\n",
    "# try it out!\n",
    "summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Part 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TextBlob Demo: \"Simplified Text Processing\"\n",
    "# Installation: pip install textblob\n",
    "! pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob, Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# identify words and noun phrases\n",
    "blob = TextBlob('Greg and Adrian are instructors for General Assembly')\n",
    "blob.words\n",
    "blob.noun_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sentiment analysis\n",
    "blob = TextBlob('I hate this horrible movie. This movie is not very good.')\n",
    "blob.sentences\n",
    "blob.sentiment.polarity\n",
    "[sent.sentiment.polarity for sent in blob.sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sentiment subjectivity\n",
    "TextBlob(\"I am a cool person\").sentiment.subjectivity # Pretty subjective\n",
    "TextBlob(\"I am a person\").sentiment.subjectivity # Pretty objective\n",
    "# different scores for essentially the same sentence\n",
    "print TextBlob('Greg and Adrian are instructors for General Assembly in Sydney').sentiment.subjectivity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# singularize and pluralize\n",
    "blob = TextBlob('Put away the dishes.')\n",
    "[word.singularize() for word in blob.words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[word.pluralize() for word in blob.words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# spelling correction\n",
    "blob = TextBlob('15 minuets late')\n",
    "blob.correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# spellcheck\n",
    "Word('parot').spellcheck()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# definitions\n",
    "Word('bank').define()\n",
    "Word('bank').define('v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# translation and language identification\n",
    "blob = TextBlob('Welcome to the classroom.')\n",
    "blob.translate(to='es')\n",
    "blob = TextBlob('Hola amigos')\n",
    "blob.detect_language()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
